{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb299f82-f4b1-4f85-9195-07724f6dd4cb",
   "metadata": {},
   "source": [
    "# Data Ingest in Python (Azure AI Search)\n",
    "提供している一連のサンプルノートブックのコンテンツを実行させるために必要な Azure AI Search の検索インデックスを作成します。このノートブックでは以下のような処理を行います。\n",
    "\n",
    "- Azure AI Search のインデックスを作成\n",
    "- Azure Blob Storage に PDF をアップロード\n",
    "- Azure AI Document Intellingence で PDF の中身を抽出して構造化\n",
    "- 抽出したテキストをチャンクに分割\n",
    "- 分割されたチャンクを Embeddings に変換\n",
    "- Azure AI Search のインデックスとして登録\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c68a5b",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "この Python サンプルを実行するには、以下が必要です：\n",
    "- [Azure AI Search リソース](https://learn.microsoft.com/azure/search/search-create-service-portal)。エンドポイントと管理者 API キーが必要です。本ノートブックは `azure-search-documents==11.4.0` 専用です。\n",
    "- [Azure AI Document Intelligence リソース](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource)。エンドポイントとキーが必要です。\n",
    "- [Azure Blob Storage リソース](https://learn.microsoft.com/azure/storage/common/storage-account-create?tabs=azure-portal)。[接続文字列](https://learn.microsoft.com/azure/storage/common/storage-account-get-info?tabs=portal#get-a-connection-string-for-the-storage-account)が必要です。\n",
    "- [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal) にアクセスできる承認済み Azure サブスクリプション\n",
    "- Azure OpenAI Service への `text-embedding-ada-002` [Embeddings モデル](https://learn.microsoft.com/azure/ai-services/openai/tutorials/embeddings?tabs=python%2Ccommand-line&pivots=programming-language-python)のデプロイメント。このデモでは、API バージョン `2023-05-15` を使用しています。\n",
    "- Azure OpenAI Service の接続とモデル情報\n",
    "  - OpenAI API キー\n",
    "  - OpenAI Embeddings モデルのデプロイメント名\n",
    "  - OpenAI API バージョン\n",
    "- [Python](https://www.python.org/downloads/release/python-31011/) (この手順はバージョン 3.10.x でテストされています)\n",
    "\n",
    "これらのデモには、[Visual Studio Code](https://code.visualstudio.com/download) と [Jupyter extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) を使用できます。本ノートブックは https://github.com/Azure-Samples/azure-search-openai-demo をベースにしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c8bef",
   "metadata": {},
   "source": [
    "## パッケージのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d9bbd-e2b9-451a-a872-56f19430d0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install azure-search-documents==11.4.0\n",
    "!pip install azure-identity==1.15.0\n",
    "!pip install azure-ai-formrecognizer==3.3.2\n",
    "!pip install azure-storage-blob==12.14.1\n",
    "!pip install openai[datalib]==1.3.9\n",
    "!pip install pypdf==3.17.0\n",
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ef945-8aa3-4538-8bf7-662c01bdf397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import azure.search.documents\n",
    "print(\"azure.search.documents\", azure.search.documents.__version__)\n",
    "import azure.ai.formrecognizer\n",
    "print(\"azure.ai.formrecognizer\", azure.ai.formrecognizer.__VERSION__)\n",
    "import azure.storage.blob\n",
    "print(\"azure.storage.blob\", azure.storage.blob.__version__)\n",
    "import openai\n",
    "print(\"openai\", openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524420cf",
   "metadata": {},
   "source": [
    "## 必要なライブラリと環境変数のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c62e8d-9891-4fde-a989-9bb040e1558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c014e02",
   "metadata": {},
   "source": [
    "## 接続設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77742973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Blob Storage\n",
    "azure_storage_container: str = \"content\"\n",
    "azure_blob_connection_string: str = \"<Your blob connection string>\"\n",
    "# Azure AI Search\n",
    "search_service_endpoint: str = \"<Your search service endpoint>\"\n",
    "search_service_admin_key: str = \"<Your search service admin key>\"\n",
    "index_name: str = \"gptkbindex\"\n",
    "search_analyzer_name: str = \"ja.lucene\"\n",
    "credential = AzureKeyCredential(search_service_admin_key)\n",
    "# Azure AI Document Intelligence\n",
    "document_intelligence_key: str = \"<Your document intelligence key>\"\n",
    "document_intelligence_endpoint: str = \"<Your document intelligence endpoint>\"\n",
    "document_intelligence_creds: str = AzureKeyCredential(document_intelligence_key)\n",
    "# Azure OpenAI Service\n",
    "AZURE_OPENAI_API_KEY = \"Your OpenAI API Key\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://<Your OpenAI Service>.openai.azure.com/\"\n",
    "model: str = \"embedding\"  # 自動構築時のデフォルト設定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da33291",
   "metadata": {},
   "source": [
    "## 検索インデックスの定義\n",
    "検索インデックススキーマとベクトル検索設定を作成します。\n",
    "以下の記法は `azure-search-documents==11.4.0` 専用となっていますので注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfba5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index():\n",
    "    # Create a search index\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "        SearchableField(\n",
    "            name=\"content\", type=\"Edm.String\", analyzer_name=search_analyzer_name\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            hidden=False,\n",
    "            searchable=True,\n",
    "            filterable=False,\n",
    "            sortable=False,\n",
    "            facetable=False,\n",
    "            vector_search_dimensions=1536,\n",
    "            vector_search_profile_name=\"embedding_config\",\n",
    "        ),\n",
    "        SimpleField(name=\"category\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "        SimpleField(name=\"sourcepage\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "        SimpleField(name=\"sourcefile\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "        SimpleField(name=\"metadata\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    ]\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"default\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=None,\n",
    "            keywords_fields=None,\n",
    "            content_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Configure the vector search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"hnsw_config\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"embedding_config\",\n",
    "                algorithm_configuration_name=\"hnsw_config\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    index_client = SearchIndexClient(endpoint=search_service_endpoint, credential=credential)\n",
    "    if index_name not in index_client.list_index_names():\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "            semantic_search=semantic_search,\n",
    "        )\n",
    "        print(f\"Creating {index_name} search index\")\n",
    "        result = index_client.create_or_update_index(index) \n",
    "        print(f' {result.name} created')\n",
    "    else:\n",
    "        print(f\"Search index {index_name} already exists\")\n",
    "\n",
    "def remove_from_index(filename):\n",
    "    print(f\"Removing sections from '{filename or '<all>'}' from search index '{index_name}'\")\n",
    "    search_client = SearchClient(endpoint=search_service_endpoint,\n",
    "                                    index_name=index_name,\n",
    "                                    credential=credential)\n",
    "    while True:\n",
    "        filter = None if filename is None else f\"sourcefile eq '{os.path.basename(filename)}'\"\n",
    "        r = search_client.search(\"\", filter=filter, top=1000, include_total_count=True)\n",
    "        if r.get_count() == 0:\n",
    "            break\n",
    "        r = search_client.delete_documents(documents=[{ \"id\": d[\"id\"] } for d in r])\n",
    "        print(f\"\\tRemoved {len(r)} sections from index\")\n",
    "        # It can take a few seconds for search results to reflect changes, so wait a bit\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405aa302",
   "metadata": {},
   "source": [
    "# Azure Blob Storage に PDF ファイルをアップロード\n",
    "PDF ファイルをページ単位に分割してから、Azure Blob Storage へアップロードします。これはチャット UI からのプレビュー用にも使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "def blob_name_from_file_page(filename, page = 0):\n",
    "    if os.path.splitext(filename)[1].lower() == \".pdf\":\n",
    "        return os.path.splitext(os.path.basename(filename))[0] + f\"-{page}\" + \".pdf\"\n",
    "    else:\n",
    "        return os.path.basename(filename)\n",
    "\n",
    "def upload_blobs(filename):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(azure_blob_connection_string)\n",
    "    blob_container = blob_service_client.get_container_client(azure_storage_container)\n",
    "    if not blob_container.exists():\n",
    "        blob_container.create_container()\n",
    "\n",
    "    # ファイルが PDF の場合、ページに分割し、各ページを個別の Blob としてアップロードする。\n",
    "    if os.path.splitext(filename)[1].lower() == \".pdf\":\n",
    "        reader = PdfReader(filename)\n",
    "        pages = reader.pages\n",
    "        for i in range(len(pages)):\n",
    "            blob_name = blob_name_from_file_page(filename, i)\n",
    "            \n",
    "            f = io.BytesIO()\n",
    "            writer = PdfWriter()\n",
    "            writer.add_page(pages[i])\n",
    "            writer.write(f)\n",
    "            f.seek(0)\n",
    "            blob_container.upload_blob(blob_name, f, overwrite=True)\n",
    "    else:\n",
    "        blob_name = blob_name_from_file_page(filename)\n",
    "        with open(filename,\"rb\") as data:\n",
    "            blob_container.upload_blob(blob_name, data, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17044ffb",
   "metadata": {},
   "source": [
    "# Azure AI Document Intelligence を利用して帳票を OCR\n",
    "PDF の OCR に用いる[レイアウトモデル](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-layout) `prebuilt-layout` は、高度な機械学習ベースのドキュメント分析 API です。これを使用すると、さまざまな形式のドキュメントを受け取り、ドキュメントの構造化されたデータ表現を返すことができます。これは、Microsoft の強力な光学式文字認識 (OCR) 機能の強化バージョンと、ディープラーニング モデルを組み合わせ、テキスト、テーブル、選択マーク、ドキュメント構造を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import html\n",
    "import jsonpickle\n",
    "\n",
    "def table_to_html(table):\n",
    "    table_html = \"<table>\"\n",
    "    rows = [sorted([cell for cell in table.cells if cell.row_index == i], key=lambda cell: cell.column_index) for i in range(table.row_count)]\n",
    "    for row_cells in rows:\n",
    "        table_html += \"<tr>\"\n",
    "        for cell in row_cells:\n",
    "            tag = \"th\" if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\") else \"td\"\n",
    "            cell_spans = \"\"\n",
    "            if cell.column_span > 1: cell_spans += f\" colSpan={cell.column_span}\"\n",
    "            if cell.row_span > 1: cell_spans += f\" rowSpan={cell.row_span}\"\n",
    "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\"\n",
    "        table_html +=\"</tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    return table_html\n",
    "\n",
    "def get_document_text(filename):\n",
    "    offset = 0\n",
    "    page_map = []\n",
    "\n",
    "    print(f\"Extracting text from '{filename}' using Azure AI Document Intelligence\")\n",
    "    form_recognizer_client = DocumentAnalysisClient(endpoint=document_intelligence_endpoint, credential=document_intelligence_creds, headers={\"x-ms-useragent\": \"azure-search-chat-demo/1.0.0\"})\n",
    "    with open(filename, \"rb\") as f:\n",
    "        poller = form_recognizer_client.begin_analyze_document(\"prebuilt-layout\", document = f)\n",
    "    form_recognizer_results = poller.result()\n",
    "    \n",
    "    # Debug 用(AnalyzeResultオブジェクト構造をそのままJSONへ)\n",
    "    # json_data = jsonpickle.encode(form_recognizer_results)\n",
    "    # with open('data.json', \"w\", encoding='utf-8') as f:\n",
    "    #     f.write(json_data)\n",
    "    #\n",
    "    # f = open(\"data.json\")\n",
    "    # json_str = f.read()\n",
    "    # form_recognizer_results = jsonpickle.decode(json_str)\n",
    "\n",
    "    for page_num, page in enumerate(form_recognizer_results.pages):\n",
    "        tables_on_page = [table for table in form_recognizer_results.tables if table.bounding_regions[0].page_number == page_num + 1]\n",
    "\n",
    "        # mark all positions of the table spans in the page\n",
    "        page_offset = page.spans[0].offset\n",
    "        page_length = page.spans[0].length\n",
    "        table_chars = [-1]*page_length\n",
    "        for table_id, table in enumerate(tables_on_page):\n",
    "            for span in table.spans:\n",
    "                # replace all table spans with \"table_id\" in table_chars array\n",
    "                for i in range(span.length):\n",
    "                    idx = span.offset - page_offset + i\n",
    "                    if idx >=0 and idx < page_length:\n",
    "                        table_chars[idx] = table_id\n",
    "\n",
    "        # build page text by replacing characters in table spans with table html\n",
    "        page_text = \"\"\n",
    "        added_tables = set()\n",
    "        for idx, table_id in enumerate(table_chars):\n",
    "            if table_id == -1:\n",
    "                page_text += form_recognizer_results.content[page_offset + idx]\n",
    "            elif table_id not in added_tables:\n",
    "                page_text += table_to_html(tables_on_page[table_id])\n",
    "                added_tables.add(table_id)\n",
    "\n",
    "        page_text += \" \"\n",
    "        page_map.append((page_num, offset, page_text))\n",
    "        offset += len(page_text)\n",
    "\n",
    "    return page_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8c11d",
   "metadata": {},
   "source": [
    "# Embeddings の生成関数の定義\n",
    "\n",
    "tenacity ライブラリを用いて Embeddings API のコールにリトライを設定することで Rate limit に対処します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = AZURE_OPENAI_API_KEY,  \n",
    "  api_version = \"2023-05-15\",\n",
    "  azure_endpoint = AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "def before_retry_sleep(retry_state):\n",
    "    print(\"Rate limited on the OpenAI embeddings API, sleeping before retrying...\")\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=15, max=60), stop=stop_after_attempt(15), before_sleep=before_retry_sleep)\n",
    "def compute_embedding(text):\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ea53b",
   "metadata": {},
   "source": [
    "# チャンク分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 1000\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "\n",
    "def split_text(page_map, filename):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]\n",
    "    print(f\"Splitting '{filename}' into sections\")\n",
    "\n",
    "    def find_page(offset):\n",
    "        num_pages = len(page_map)\n",
    "        for i in range(num_pages - 1):\n",
    "            if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "                return i\n",
    "        return num_pages - 1\n",
    "\n",
    "    all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        yield (section_text, find_page(start))\n",
    "\n",
    "        last_table_start = section_text.rfind(\"<table\")\n",
    "        if (last_table_start > 2 * SENTENCE_SEARCH_LIMIT and last_table_start > section_text.rfind(\"</table\")):\n",
    "            # If the section ends with an unclosed table, we need to start the next section with the table.\n",
    "            # If table starts inside SENTENCE_SEARCH_LIMIT, we ignore it, as that will cause an infinite loop for tables longer than MAX_SECTION_LENGTH\n",
    "            # If last table starts inside SECTION_OVERLAP, keep overlapping\n",
    "            print(f\"Section ends with unclosed table, starting next section with the table at page {find_page(start)} offset {start} table start {last_table_start}\")\n",
    "            start = min(end - SECTION_OVERLAP, start + last_table_start)\n",
    "        else:\n",
    "            start = end - SECTION_OVERLAP\n",
    "\n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], find_page(start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ca920",
   "metadata": {},
   "source": [
    "# インデックスに登録するドキュメントを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca710ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "import json\n",
    "def filename_to_id(filename):\n",
    "    filename_ascii = re.sub(\"[^0-9a-zA-Z_-]\", \"_\", filename)\n",
    "    filename_hash = base64.b16encode(filename.encode('utf-8')).decode('ascii')\n",
    "    return f\"file-{filename_ascii}-{filename_hash}\"\n",
    "\n",
    "def create_sections(filename, page_map, use_vectors, category):\n",
    "    file_id = filename_to_id(filename)\n",
    "    for i, (content, pagenum) in enumerate(split_text(page_map, filename)):\n",
    "        section = {\n",
    "            \"id\": f\"{file_id}-page-{i}\",\n",
    "            \"content\": content,\n",
    "            \"category\": category,\n",
    "            \"sourcepage\": blob_name_from_file_page(filename, pagenum),\n",
    "            \"sourcefile\": filename,\n",
    "            \"metadata\": json.dumps({\"page\": pagenum, \"sourcepage\": blob_name_from_file_page(filename, pagenum)})\n",
    "        }\n",
    "        \n",
    "        section[\"embedding\"] = compute_embedding(content)\n",
    "        yield section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f93338",
   "metadata": {},
   "source": [
    "# チャンクをインデックス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ed990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_sections(filename, sections):\n",
    "    search_client = SearchClient(\n",
    "        endpoint=search_service_endpoint, index_name=index_name, credential=credential\n",
    "    )\n",
    "    i = 0\n",
    "    batch = []\n",
    "    for s in sections:\n",
    "        batch.append(s)\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            results = search_client.upload_documents(documents=batch)\n",
    "            succeeded = sum([1 for r in results if r.succeeded])\n",
    "            print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")\n",
    "            batch = []\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        results = search_client.upload_documents(documents=batch)\n",
    "        succeeded = sum([1 for r in results if r.succeeded])\n",
    "        print(f\"\\tIndexed {len(results)} sections, {succeeded} succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b7c94",
   "metadata": {},
   "source": [
    "# メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print(\"Create Search Index...\")\n",
    "create_search_index()\n",
    "print(\"Processing files...\")\n",
    "\n",
    "path_pattern = \"../data/*.pdf\"\n",
    "for filename in glob.glob(path_pattern):\n",
    "    print(f\"Processing '{filename}'\")\n",
    "    try:\n",
    "        upload_blobs(filename)\n",
    "        remove_from_index(filename)\n",
    "        page_map = get_document_text(filename)\n",
    "        category = os.path.basename(os.path.dirname(filename))\n",
    "        sections = create_sections(\n",
    "            os.path.basename(filename), page_map, False, category\n",
    "        )\n",
    "        index_sections(os.path.basename(filename), sections)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\tGot an error while reading {filename} -> {e} --> skipping file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
